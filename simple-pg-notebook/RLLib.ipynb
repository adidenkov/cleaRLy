{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install RLLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May need to restart the kernel if anything new is installed\n",
    "%pip install ray[rllib]\n",
    "%pip install pettingzoo[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this succeeds, your rllib / pettingzoo install should be all set\n",
    "# Make sure you also have pytorch or tensorflow working\n",
    "from ray import tune\n",
    "from ray.rllib.examples.env.rock_paper_scissors import RockPaperScissors\n",
    "from ray.rllib.examples.policy.rock_paper_scissors_dummies import AlwaysSameHeuristic\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "\n",
    "from gym.spaces import Discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal MARL Example\n",
    "## Trivial agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rock-paper-scissors with template agent\n",
    "config = {\n",
    "    \"env\": RockPaperScissors,\n",
    "    \"framework\": \"torch\",\n",
    "    \"num_envs_per_worker\": 4,\n",
    "}\n",
    "\n",
    "# Iterate until either is achieved\n",
    "stop = {\n",
    "    \"training_iteration\": 150,\n",
    "    \"timesteps_total\": 100000,\n",
    "    \"episode_reward_mean\": 1000.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.1/8.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/1.51 GiB heap, 0.0/0.49 GiB objects<br>Result logdir: C:\\Users\\Penguin\\ray_results\\PG<br>Number of trials: 1/1 (1 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-06 01:46:12,862\tINFO tune.py:448 -- Total run time: 29.99 seconds (20.48 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# Train the policy\n",
    "results = tune.run(\"PG\", config=config, stop=stop, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Modify faulty source code (not part of the agent implementation)\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.policy.view_requirement import ViewRequirement\n",
    "\n",
    "class AlwaysSameHeuristic(Policy):\n",
    "    \"\"\"Pick a random move and stick with it for the entire episode.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # [Force-removed exploration - was breaking without tensorflow]\n",
    "        self.view_requirements.update({\n",
    "            \"state_in_0\": ViewRequirement(\n",
    "                \"state_out_0\",\n",
    "                shift=-1,\n",
    "                space=gym.spaces.Box(0, 100, shape=(), dtype=np.int32))\n",
    "        })\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return [\n",
    "            random.choice([\n",
    "                RockPaperScissors.ROCK, RockPaperScissors.PAPER,\n",
    "                RockPaperScissors.SCISSORS\n",
    "            ])\n",
    "        ]\n",
    "\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches=None,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "        return state_batches[0], state_batches, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rock-paper-scissors against a deterministic agent\n",
    "config = {\n",
    "    \"env\": RockPaperScissors,\n",
    "    \"env_config\": {\n",
    "        \"sheldon_cooper\": False,\n",
    "    },\n",
    "    \"num_gpus\": 1,\n",
    "    \"num_envs_per_worker\": 4,\n",
    "    \"rollout_fragment_length\": 10,\n",
    "    \"multiagent\": {\n",
    "        \"policies_to_train\": [\"learned\"],\n",
    "        \"policies\": {\n",
    "            \"always_same\": (AlwaysSameHeuristic, Discrete(3), Discrete(3), {}),\n",
    "            \"learned\": (None, Discrete(3), Discrete(3), {\"framework\": \"torch\"}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": lambda p: \"learned\" if p == \"player1\" else \"always_same\"\n",
    "    },\n",
    "    \"framework\": \"torch\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learned': -0.4, 'always_same': 0.4}\n",
      "{'learned': -0.2, 'always_same': 0.2}\n",
      "{'learned': 0.11666666666666667, 'always_same': -0.11666666666666667}\n",
      "{'learned': 0.275, 'always_same': -0.275}\n",
      "{'learned': 0.43, 'always_same': -0.43}\n",
      "{'learned': 0.85, 'always_same': -0.85}\n",
      "{'learned': 1.22, 'always_same': -1.22}\n",
      "{'learned': 1.55, 'always_same': -1.55}\n",
      "{'learned': 1.98, 'always_same': -1.98}\n",
      "{'learned': 2.22, 'always_same': -2.22}\n",
      "{'learned': 2.55, 'always_same': -2.55}\n",
      "{'learned': 2.98, 'always_same': -2.98}\n",
      "{'learned': 3.35, 'always_same': -3.35}\n",
      "{'learned': 3.94, 'always_same': -3.94}\n",
      "{'learned': 4.69, 'always_same': -4.69}\n",
      "{'learned': 5.43, 'always_same': -5.43}\n",
      "{'learned': 5.99, 'always_same': -5.99}\n",
      "{'learned': 6.53, 'always_same': -6.53}\n",
      "{'learned': 6.96, 'always_same': -6.96}\n",
      "{'learned': 7.45, 'always_same': -7.45}\n",
      "{'learned': 7.8, 'always_same': -7.8}\n",
      "{'learned': 8.15, 'always_same': -8.15}\n",
      "{'learned': 8.58, 'always_same': -8.58}\n",
      "{'learned': 8.78, 'always_same': -8.78}\n",
      "{'learned': 8.95, 'always_same': -8.95}\n",
      "{'learned': 8.97, 'always_same': -8.97}\n",
      "{'learned': 9.15, 'always_same': -9.15}\n",
      "{'learned': 9.23, 'always_same': -9.23}\n",
      "{'learned': 9.35, 'always_same': -9.35}\n",
      "{'learned': 9.4, 'always_same': -9.4}\n",
      "{'learned': 9.57, 'always_same': -9.57}\n",
      "{'learned': 9.66, 'always_same': -9.66}\n",
      "{'learned': 9.67, 'always_same': -9.67}\n",
      "{'learned': 9.73, 'always_same': -9.73}\n"
     ]
    }
   ],
   "source": [
    "# Iterate until either of the stopping criteria\n",
    "def train(trainer, env):\n",
    "    for _ in range(150):\n",
    "        results = trainer.train()\n",
    "        print(results[\"policy_reward_mean\"])\n",
    "        if results[\"timesteps_total\"] > 100000:\n",
    "            break\n",
    "        elif env.player1_score - env.player2_score > 1000.0:\n",
    "            return\n",
    "    raise ValueError(\n",
    "        \"Desired reward difference ({}) not reached! Only got to {}.\".\n",
    "        format(args.stop_reward, env.player1_score - env.player2_score))\n",
    "\n",
    "# Train the policy\n",
    "cls = get_agent_class(\"PG\")\n",
    "trainer = cls(config=config)\n",
    "env = trainer.workers.local_worker().env\n",
    "train(trainer, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.11 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python361164bitpytorchconda8d4a1aaaa9bc45b48bd060d59b8c15d4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
