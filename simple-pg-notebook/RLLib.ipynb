{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install RLLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May need to restart the kernel if anything new is installed\n",
    "%pip install ray[rllib]\n",
    "%pip install pettingzoo[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this succeeds, your rllib / pettingzoo install should be all set\n",
    "# Make sure you also have pytorch or tensorflow working\n",
    "from ray import tune\n",
    "from ray.rllib.examples.env.rock_paper_scissors import RockPaperScissors\n",
    "from ray.rllib.examples.policy.rock_paper_scissors_dummies import AlwaysSameHeuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal MARL Example\n",
    "## Trivial agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rock-paper-scissors with template agent\n",
    "config = {\n",
    "    \"env\": RockPaperScissors,\n",
    "    \"framework\": \"torch\",\n",
    "    \"num_envs_per_worker\": 4,\n",
    "}\n",
    "\n",
    "# Iterate until either is achieved\n",
    "stop = {\n",
    "    \"training_iteration\": 150,\n",
    "    \"timesteps_total\": 100000,\n",
    "    \"episode_reward_mean\": 1000.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.9/8.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/1.37 GiB heap, 0.0/0.44 GiB objects<br>Result logdir: C:\\Users\\***\\ray_results\\PG<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_RockPaperScissors_adba3_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">          25.808</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                10</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 17:52:35,824\tINFO tune.py:439 -- Total run time: 40.91 seconds (30.63 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# Train the policy\n",
    "results = tune.run(\"PG\", config=config, stop=stop, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple policies (does not work yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Top sneaky: remove faulty code (this cell should be hidden)\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from ray.rllib.examples.env.rock_paper_scissors import RockPaperScissors\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.policy.view_requirement import ViewRequirement\n",
    "\n",
    "class AlwaysSameHeuristic(Policy):\n",
    "    \"\"\"Pick a random move and stick with it for the entire episode.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # [Force-removed exploration - was breaking without tensorflow]\n",
    "        self.view_requirements.update({\n",
    "            \"state_in_0\": ViewRequirement(\n",
    "                \"state_out_0\",\n",
    "                shift=-1,\n",
    "                space=gym.spaces.Box(0, 100, shape=(), dtype=np.int32))\n",
    "        })\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return [\n",
    "            random.choice([\n",
    "                RockPaperScissors.ROCK, RockPaperScissors.PAPER,\n",
    "                RockPaperScissors.SCISSORS\n",
    "            ])\n",
    "        ]\n",
    "\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches=None,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "        return state_batches[0], state_batches, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rock-paper-scissors against a deterministic agent\n",
    "config = {\n",
    "    \"env\": RockPaperScissors,\n",
    "    \"env_config\": {\n",
    "        \"sheldon_cooper\": False,\n",
    "    },\n",
    "    \"num_gpus\": 1,\n",
    "    \"num_envs_per_worker\": 4,\n",
    "    \"rollout_fragment_length\": 10,\n",
    "    \"multiagent\": {\n",
    "        \"policies_to_train\": [\"learned\"],\n",
    "        \"policies\": {\n",
    "            \"always_same\": (AlwaysSameHeuristic, Discrete(3), Discrete(3), {}),\n",
    "            \"learned\": (None, Discrete(3), Discrete(3), {\"framework\": \"torch\"}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": lambda p: \"learned\" if p == \"player1\" else \"always_same\"\n",
    "    },\n",
    "    \"framework\": \"torch\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate until either of the stopping criteria\n",
    "def train(trainer, env):\n",
    "    for _ in range(150):\n",
    "        results = trainer.train()\n",
    "        print(results[\"episode_reward_mean\"])\n",
    "        if results[\"timesteps_total\"] > 100000:\n",
    "            break\n",
    "        elif env.player1_score - env.player2_score > 1000.0:\n",
    "            return\n",
    "    raise ValueError(\n",
    "        \"Desired reward difference ({}) not reached! Only got to {}.\".\n",
    "        format(args.stop_reward, env.player1_score - env.player2_score))\n",
    "\n",
    "# Train the policy\n",
    "cls = get_agent_class(\"PG\")\n",
    "trainer = cls(config=config)\n",
    "env = trainer.workers.local_worker().env\n",
    "train(trainer, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.11 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python361164bitpytorchconda8d4a1aaaa9bc45b48bd060d59b8c15d4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
