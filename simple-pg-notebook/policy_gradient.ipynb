{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "import model as m\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "from collections import deque\n",
    "import statistics\n",
    "from visualize import update_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple gym environment\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy for choosing actions\n",
    "policy = m.DiscretePolicy(\n",
    "    n_obs = env.observation_space.shape[0],\n",
    "    n_acts = env.action_space.n,\n",
    "    n_hidden = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for training the policy\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward = 0\n",
    "last_episode_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.45\n",
      "22.1\n",
      "22.91\n",
      "24.28\n",
      "22.55\n",
      "23.14\n",
      "25.02\n",
      "29.24\n",
      "32.01\n",
      "33.85\n",
      "40.75\n",
      "43.3\n",
      "55.53\n",
      "62.37\n",
      "69.81\n",
      "98.64\n",
      "120.42\n",
      "142.74\n",
      "161.08\n",
      "177.47\n",
      "206.17\n",
      "209.83\n",
      "235.42\n",
      "196.59\n",
      "167.91\n",
      "154.41\n",
      "287.97\n",
      "345.1\n",
      "373.79\n",
      "446.65\n",
      "468.67\n",
      "468.76\n",
      "463.14\n",
      "445.86\n",
      "371.63\n",
      "329.56\n",
      "349.66\n",
      "407.48\n",
      "416.11\n",
      "455.89\n",
      "467.58\n",
      "487.07\n",
      "487.93\n",
      "475.31\n",
      "456.11\n",
      "399.36\n",
      "447.96\n",
      "484.82\n",
      "472.38\n",
      "486.07\n",
      "499.38\n",
      "499.81\n",
      "496.65\n",
      "487.93\n",
      "492.06\n",
      "484.07\n",
      "494.73\n",
      "498.28\n",
      "498.23\n",
      "493.62\n",
      "485.24\n",
      "460.76\n",
      "448.67\n",
      "458.62\n",
      "478.77\n",
      "486.48\n",
      "490.16\n",
      "493.49\n",
      "494.24\n",
      "496.13\n",
      "498.87\n",
      "493.7\n",
      "474.61\n",
      "486.96\n",
      "497.17\n",
      "498.1\n",
      "479.11\n",
      "475.44\n",
      "480.95\n",
      "475.1\n",
      "484.61\n",
      "495.73\n",
      "496.46\n",
      "496.29\n",
      "498.83\n",
      "494.97\n",
      "492.33\n",
      "483.83\n",
      "471.4\n",
      "458.59\n",
      "435.74\n",
      "473.32\n",
      "461.49\n",
      "456.45\n",
      "459.91\n",
      "464.01\n",
      "467.89\n",
      "435.89\n",
      "456.91\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10000):\n",
    "    # Part 1: gather experience\n",
    "    \n",
    "    trajectories = []\n",
    "    state = env.reset()\n",
    "\n",
    "    # Display and plot recent performance\n",
    "    if len(last_episode_rewards) == 100:\n",
    "        avg_ep_reward = statistics.mean(last_episode_rewards)\n",
    "        print(avg_ep_reward)\n",
    "        update_viz(episode, avg_ep_reward)\n",
    "        last_episode_rewards.clear()\n",
    "\n",
    "    # Run a simulation until completion\n",
    "    while len(trajectories) == 0 or not trajectories[-1][\"done\"]:\n",
    "        action = policy(torch.tensor(state, dtype=torch.float32))\n",
    "        new_state, reward, done, _ = env.step(action.item())\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Record data to replay memory\n",
    "        trajectories.append({\n",
    "            \"state\": state,\n",
    "            \"action\": action,\n",
    "            \"reward\": torch.tensor([reward]),\n",
    "            \"done\": done\n",
    "        })\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    last_episode_rewards.append(episode_reward)\n",
    "    episode_reward = 0\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        # Part 2: learn from experience\n",
    "        \n",
    "        states = torch.tensor([trajectory[\"state\"] for trajectory in trajectories], dtype=torch.float32)\n",
    "        actions = torch.tensor([trajectory[\"action\"] for trajectory in trajectories], dtype=torch.float32)\n",
    "        dones = torch.tensor([torch.tensor([1.0]) if trajectory[\"done\"] else torch.tensor([0.0]) for trajectory in trajectories], dtype=torch.float32)\n",
    "        rewards = [trajectory[\"reward\"] for trajectory in trajectories]\n",
    "\n",
    "        # Compute return, i.e. cumulative reward\n",
    "        returns = [0] * len(rewards)\n",
    "        discounted_future = 0\n",
    "        \n",
    "        for i in reversed(range(len(rewards))):\n",
    "            if dones[i]:\n",
    "                returns[i] = rewards[i]\n",
    "            else:\n",
    "                returns[i] = rewards[i] + discounted_future\n",
    "            \n",
    "            discounted_future = returns[i] * 0.99\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "\n",
    "        # Compute the loss function\n",
    "        mean = returns.mean()\n",
    "        std = returns.std() + 1e-6\n",
    "        returns = (returns - mean)/std\n",
    "        \n",
    "        log_probs = policy.log_prob(states, actions)\n",
    "\n",
    "        policy_loss = -(torch.dot(returns, log_probs)).mean()\n",
    "\n",
    "        # Train through backpropagation on the loss\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a rendered simulation\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = policy(torch.tensor(state, dtype=torch.float32)).item()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
